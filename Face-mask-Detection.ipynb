{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask labelled 'yes': 690\n",
      "The number of images with facemask labelled 'no': 668\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('data/with_mask')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('data/without_mask')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 951 images belonging to 2 classes.\n",
      "Found 407 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"data\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.3)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150),\n",
    "                                                   subset = 'training')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150),\n",
    "                                                            subset = 'validation' )\n",
    "checkpoint = ModelCheckpoint('best.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "95/96 [============================>.] - ETA: 1s - loss: 0.8472 - acc: 0.5058WARNING:tensorflow:From C:\\Users\\sampr\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: best.model\\assets\n",
      "96/96 [==============================] - 146s 2s/step - loss: 0.8456 - acc: 0.5026 - val_loss: 0.6961 - val_acc: 0.5061\n",
      "Epoch 2/5\n",
      "95/96 [============================>.] - ETA: 1s - loss: 0.6725 - acc: 0.5845INFO:tensorflow:Assets written to: best.model\\assets\n",
      "96/96 [==============================] - 140s 1s/step - loss: 0.6729 - acc: 0.5825 - val_loss: 0.4990 - val_acc: 0.7740\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 129s 1s/step - loss: 0.5827 - acc: 0.6866 - val_loss: 0.6739 - val_acc: 0.5774\n",
      "Epoch 4/5\n",
      "95/96 [============================>.] - ETA: 1s - loss: 0.5959 - acc: 0.6961INFO:tensorflow:Assets written to: best.model\\assets\n",
      "96/96 [==============================] - 135s 1s/step - loss: 0.5940 - acc: 0.6982 - val_loss: 0.4309 - val_acc: 0.8796\n",
      "Epoch 5/5\n",
      "95/96 [============================>.] - ETA: 1s - loss: 0.3699 - acc: 0.8735INFO:tensorflow:Assets written to: best.model\\assets\n",
      "96/96 [==============================] - 136s 1s/step - loss: 0.3697 - acc: 0.8738 - val_loss: 0.3151 - val_acc: 0.8771\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=5,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])\n",
    "# model = load_model('model-017.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_dict={0:'without mask',1:'with mask'}\n",
    "color_dict={0:(100,60,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
